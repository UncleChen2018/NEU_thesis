{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5d1979-f1d1-4915-b2f9-c62de5a70b1e",
   "metadata": {},
   "source": [
    "# Script - to test all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1808f56-78d9-4b39-9641-54605578196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import zstandard as zstd\n",
    "import matplotlib.pyplot as plt\n",
    "from chronos_utils import (\n",
    "    rolling_forecast_and_reconstruct,\n",
    "    compute_error_metrics,\n",
    "    plot_error_distribution,\n",
    "    plot_time_series_with_forecast,\n",
    "    decompress_to_csv,\n",
    "    check_data_valid,\n",
    "    check_file_valid,\n",
    "    compress_to_file,\n",
    "    decompress_to_mem,\n",
    "    compress_to_file_debug,\n",
    "    LinearQuantizer,\n",
    "    decompress_data\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import zstandard as zstd\n",
    "\n",
    "def naive_compress_with_zstd(data, EB):\n",
    "\n",
    "    # Perform naive quantization\n",
    "    naive_quantile = ((data / EB).astype('int64') * EB).astype('float32')\n",
    "\n",
    "    # Check if quantization is within acceptable limits (Optional, for validation)\n",
    "    is_close = np.allclose(naive_quantile, data, atol=EB)\n",
    "    if not is_close:\n",
    "        print(f\"All elements in naive zstd are not within the error bound ({EB}): {is_close}\")\n",
    "\n",
    "    # Compress the data using zstandard with a high compression level\n",
    "    compressor = zstd.ZstdCompressor(level=22)\n",
    "    compressed_data = compressor.compress(naive_quantile.tobytes())\n",
    "\n",
    "    return len(compressed_data)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def compress_with_sz3(array, EB, verbose=False):\n",
    "    # Write the bytes to a binary file\n",
    "    temp_input_filename = 'sz_temp.bin'\n",
    "    temp_output_filename = 'sz_temp.bin.sz'\n",
    "    \n",
    "    # Convert the numpy array to bytes and write to file\n",
    "    array.tofile(temp_input_filename)\n",
    "    \n",
    "    # Prepare the SZ3 command based on the dtype of the array\n",
    "    if array.dtype == np.float32:\n",
    "        dtype_flag = '-f'\n",
    "    elif array.dtype == np.float64:\n",
    "        dtype_flag = '-d'\n",
    "    elif array.dtype == np.int32:\n",
    "        dtype_flag = '-I 32'\n",
    "    elif array.dtype == np.int64:\n",
    "        dtype_flag = '-I 64'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type for SZ3 compression\")\n",
    "\n",
    "    # Construct the command\n",
    "    command = f\"./sz3 {dtype_flag} -i {temp_input_filename} -z {temp_output_filename} -1 {array.size} -M ABS {EB}\"\n",
    "    \n",
    "    # Execute the command with or without output\n",
    "    if verbose:\n",
    "        print(\"Executing command:\", command)\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "    else:\n",
    "        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    \n",
    "    # Get the size of the compressed file\n",
    "    compressed_size = os.path.getsize(temp_output_filename)\n",
    "    \n",
    "    # Remove the temporary files\n",
    "    os.remove(temp_input_filename)\n",
    "    os.remove(temp_output_filename)\n",
    "    \n",
    "    return compressed_size\n",
    "\n",
    "\n",
    "def compress_array_with_zstd(array):\n",
    "    \"\"\" Compresses a numpy array using Zstandard and returns the size of the compressed data. \"\"\"\n",
    "    compressor = zstd.ZstdCompressor(level=22)\n",
    "    # Convert the numpy array to bytes and compress\n",
    "    compressed_data = compressor.compress(array.tobytes())\n",
    "    return len(compressed_data)\n",
    "\n",
    "\n",
    "def calculate_entropy_with_scipy(data):\n",
    "    # Count the frequency of each unique value\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    # Calculate the entropy using scipy.stats.entropy\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rrmse(error, actual):\n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        return float('inf')\n",
    "    squared_relative_errors = (error / mean_actual) ** 2\n",
    "    return np.sqrt(np.mean(squared_relative_errors))\n",
    "\n",
    "def calculate_rmse(error, actual):\n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        return float('inf')\n",
    "    squared_relative_errors = (error / mean_actual) ** 2\n",
    "    return np.mean(squared_relative_errors)\n",
    "\n",
    "def calculate_rmae(error, actual):\n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        return float('inf')\n",
    "    absolute_relative_errors = np.abs(error / mean_actual)\n",
    "    return np.sqrt(np.mean(absolute_relative_errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5263854-3832-4c4c-b8b4-4d3f2577c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Define constants\n",
    "DATA_FOLDER = 'experiment_export_data'\n",
    "BIN_FOLDER = 'bin_data'\n",
    "COMPRESSED_FOLDER = 'compressed_data'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(BIN_FOLDER, exist_ok=True)\n",
    "os.makedirs(COMPRESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def perform_compression_tests(data_file, CTX, PDT, EB, model, test_len=None, single_mode=True):\n",
    "\n",
    "    bin_file = f\"{BIN_FOLDER}/{data_file}.bin\"\n",
    "    csv_file = f\"{DATA_FOLDER}/{data_file}.csv\"\n",
    "    unique_name = f\"{data_file}_model{model}_EB{str(EB)}_CTX{CTX}_PDT{PDT}.tar.gz\"\n",
    "    compressed_file = f\"{COMPRESSED_FOLDER}/{unique_name}\"\n",
    "\n",
    "    try:\n",
    "        df = np.fromfile(bin_file, dtype=np.float32)\n",
    "        if test_len is not None:\n",
    "            df = df[:test_len]\n",
    "\n",
    "        # Begin compression testing\n",
    "        start_time = time.time()\n",
    "        sz3_size = compress_with_sz3(df, EB)\n",
    "        mid_time = time.time()\n",
    "        zstd_size = naive_compress_with_zstd(df, EB)\n",
    "        end_time = time.time()\n",
    "\n",
    "        compression_time_sz3 = mid_time - start_time\n",
    "        compression_time_zstd = end_time - mid_time\n",
    "\n",
    "        # Debug compression\n",
    "        start_time = time.time()\n",
    "        forecast, error = compress_to_file_debug(df, compressed_file, CTX, PDT, EB, model)\n",
    "        mid_time = time.time()\n",
    "        \n",
    "        decompression_time = 0\n",
    "        if not single_mode:\n",
    "            de_data, prediction, context, recovered_error = decompress_to_mem(compressed_file)\n",
    "            end_time = time.time()\n",
    "            decompression_time = end_time - mid_time\n",
    "  \n",
    "\n",
    "        compression_time_debug = mid_time - start_time\n",
    "  \n",
    "\n",
    "        # Calculate additional metrics\n",
    "        df_segment = df[CTX:]\n",
    "        df_entropy = calculate_entropy_with_scipy(df_segment)\n",
    "        df_min = np.min(df_segment)\n",
    "        df_max = np.max(df_segment)\n",
    "        df_mean = np.mean(df_segment)\n",
    "        df_variance = np.var(df_segment)\n",
    "\n",
    "        error_entropy = calculate_entropy_with_scipy(error)\n",
    "        error_min = np.min(error)\n",
    "        error_max = np.max(error)\n",
    "        error_mean = np.mean(error)\n",
    "        error_variance = np.var(error)\n",
    "\n",
    "        mean_absolute_error = np.mean(np.abs(error - df_segment))\n",
    "        mean_squared_error = np.mean((error - df_segment)**2)\n",
    "\n",
    "        rrmse = calculate_rrmse(error, df_segment)\n",
    "        rmse = calculate_rmse(error, df_segment)\n",
    "        rmae = calculate_rmae(error, df_segment)\n",
    "\n",
    "        # File sizes for comparison\n",
    "        csv_size = os.path.getsize(csv_file)\n",
    "        bin_size = os.path.getsize(bin_file)\n",
    "        fm_size = os.path.getsize(compressed_file)\n",
    "\n",
    "        # File size for the error vs use our multi-scale way\n",
    "        zstd_error_size = compress_array_with_zstd(error)        \n",
    "        \n",
    "\n",
    "        # Data accuracy check\n",
    "        is_accurate = np.allclose(de_data, df, atol=EB) if not single_mode else None\n",
    "        \n",
    "\n",
    "        # Consolidate all results into a dictionary with clearly named keys\n",
    "        results = {\n",
    "            \"Data File\": data_file,\n",
    "            \"Model\": model,\n",
    "            \"EB\": EB,\n",
    "            \"CTX\": CTX,\n",
    "            \"PDT\": PDT,\n",
    "            \"Success\": True,\n",
    "            \"Error Message\": '',\n",
    "            \"csv_size\": csv_size,\n",
    "            \"bin_size\": bin_size,\n",
    "            \"compressed_size\": fm_size,\n",
    "            \"sz3_size\": sz3_size,\n",
    "            \"zstd_size\": zstd_size,\n",
    "            \"is_accurate\": is_accurate,\n",
    "            \"compression_time_sz3\": compression_time_sz3,\n",
    "            \"compression_time_zstd\": compression_time_zstd,\n",
    "            \"compression_time_debug\": compression_time_debug,\n",
    "            \"decompression_time\": decompression_time,\n",
    "            \"zstd_error_size\": zstd_error_size,\n",
    "            \"df_entropy\": df_entropy,\n",
    "            \"df_min\": df_min,\n",
    "            \"df_max\": df_max,\n",
    "            \"df_mean\": df_mean,\n",
    "            \"df_variance\": df_variance,\n",
    "            \"error_entropy\": error_entropy,\n",
    "            \"error_min\": error_min,\n",
    "            \"error_max\": error_max,\n",
    "            \"error_mean\": error_mean,\n",
    "            \"error_variance\": error_variance,\n",
    "            \"mean_absolute_error\": mean_absolute_error,\n",
    "            \"mean_squared_error\": mean_squared_error,\n",
    "            \"RRMSE\": rrmse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMAE\": rmae\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback for all fields if an exception occurs\n",
    "        error_details = {\n",
    "            \"Data File\": data_file,\n",
    "            \"Model\": model,\n",
    "            \"EB\": EB,\n",
    "            \"CTX\": CTX,\n",
    "            \"PDT\": PDT,\n",
    "            \"Success\": False,\n",
    "            \"Error Message\": str(e),\n",
    "            # Set all additional data fields to None\n",
    "            \"csv_size\": None,\n",
    "            \"bin_size\": None,\n",
    "            \"compressed_size\": None,\n",
    "            \"sz3_size\": None,\n",
    "            \"zstd_size\": None,\n",
    "            \"is_accurate\": None,\n",
    "            \"compression_time_sz3\": None,\n",
    "            \"compression_time_zstd\": None,\n",
    "            \"compression_time_debug\": None,\n",
    "            \"decompression_time\": None,\n",
    "            \"zstd_error_size\": None,\n",
    "            \"df_entropy\": None,\n",
    "            \"df_min\": None,\n",
    "            \"df_max\": None,\n",
    "            \"df_mean\": None,\n",
    "            \"df_variance\": None,\n",
    "            \"error_entropy\": None,\n",
    "            \"error_min\": None,\n",
    "            \"error_max\": None,\n",
    "            \"error_mean\": None,\n",
    "            \"error_variance\": None,\n",
    "            \"mean_absolute_error\": None,\n",
    "            \"mean_squared_error\": None,\n",
    "            \"RRMSE\": None,\n",
    "            \"RMSE\": None,\n",
    "            \"RMAE\": None\n",
    "        }\n",
    "        return error_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25d282d-da36-4c06-aea5-4331127f4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_experiments(data_with_precision, models, EB_values, CTX_values, PDT_values, test_len=None, results_file_path=\"compression_results.csv\", create_new_file=False):\n",
    " \n",
    "    standard_columns = [\n",
    "    'Data File', 'Model', 'EB', 'CTX', 'PDT', 'Success', 'Error Message',\n",
    "    'csv_size', 'bin_size', 'compressed_size', 'sz3_size', 'zstd_size', \n",
    "    'is_accurate', 'compression_time_sz3', 'compression_time_zstd', \n",
    "    'compression_time_debug', 'decompression_time', 'zstd_error_size', \n",
    "    'df_entropy', 'df_min', 'df_max', 'df_mean', 'df_variance', \n",
    "    'error_entropy', 'error_min', 'error_max', 'error_mean', \n",
    "    'error_variance', 'mean_absolute_error', 'mean_squared_error', \n",
    "    'RRMSE', 'RMSE', 'RMAE']\n",
    "\n",
    "\n",
    "    if create_new_file or not os.path.exists(results_file_path):\n",
    "        pd.DataFrame(columns=standard_columns).to_csv(results_file_path, index=False)\n",
    "    \n",
    "    results_df = pd.read_csv(results_file_path) if not create_new_file else pd.DataFrame(columns=standard_columns)\n",
    "\n",
    "    for data_file, mcp in data_with_precision:\n",
    "        for EB in EB_values:\n",
    "            if EB <= 10 ** -mcp:\n",
    "                print(f\"Skipping: {data_file} with EB: {EB} no greater than MCP limit (10^-{mcp})\")\n",
    "                continue\n",
    "            for CTX in CTX_values:\n",
    "                for PDT in PDT_values:\n",
    "                    if PDT > CTX:\n",
    "                        print(f\"Skipping: {data_file} with PDT: {PDT} greater than CTX: {CTX}\")\n",
    "                        continue\n",
    "                    for model in models:\n",
    "                        if experiment_already_run(data_file, model, EB, CTX, PDT, results_df):\n",
    "                            print(f\"Skipping already completed experiment: {data_file} with Model: {model}, EB: {EB}, CTX: {CTX}, PDT: {PDT}\")\n",
    "                            continue\n",
    "                        print(f\"Testing {data_file} with Model: {model}, EB: {EB}, CTX: {CTX}, PDT: {PDT}\")\n",
    "                        result = perform_compression_tests(data_file, CTX, PDT, EB, model, test_len,single_mode)\n",
    "                        pd.DataFrame([result]).to_csv(results_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "    print(\"All experiments are done and results are saved.\")\n",
    "\n",
    "def experiment_already_run(data_file, model, EB, CTX, PDT, results_df):\n",
    "    \"\"\" Check if the experiment configuration has already been tested and recorded. \"\"\"\n",
    "    return ((results_df['Data File'] == data_file) & \n",
    "            (results_df['Model'] == model) &\n",
    "            (results_df['EB'] == EB) &\n",
    "            (results_df['CTX'] == CTX) &\n",
    "            (results_df['PDT'] == PDT)).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8bd6b8-5ae0-47a9-8628-1a3de02daf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already completed experiment: smart with Model: 0, EB: 0.1, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: smart with Model: 0, EB: 0.01, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: smart with Model: 0, EB: 0.001, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: smart with Model: 0, EB: 0.0001, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: smart with Model: 0, EB: 1e-05, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: smart with Model: 0, EB: 1e-06, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 0.1, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 0.01, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 0.001, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 0.0001, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 1e-05, CTX: 256, PDT: 1\n",
      "Skipping already completed experiment: sceaux with Model: 0, EB: 1e-06, CTX: 256, PDT: 1\n",
      "Testing LOOP_SEATTLE with Model: 0, EB: 0.1, CTX: 256, PDT: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ece9cf184344f84af8c6806682851a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compress processing:   0%|          | 0/104864 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 4.47\n",
      "Testing LOOP_SEATTLE with Model: 0, EB: 0.01, CTX: 256, PDT: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edb4b277e9640e3896f8007afd6f62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compress processing:   0%|          | 0/104864 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_with_precision = [\n",
    "   #(\"london_smart_meters_with_missing\", 3),\n",
    "    (\"smart\", 7),\n",
    "    (\"sceaux\", 7),\n",
    "    (\"LOOP_SEATTLE\", 6),\n",
    "    (\"oikolab_weather\", 7),\n",
    "    (\"elecdemand\", 7),\n",
    "    (\"traffic_hourly\", 4),\n",
    "    (\"elf\", 4),\n",
    "    (\"subseasonal_precip\", 6)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "models = [0]\n",
    "# Create DP values from 0 to -6\n",
    "DP_values = list(range(-1, -7, -1))\n",
    "EB_values = [float(10**dp) for dp in DP_values]\n",
    "CTX_values = [256]\n",
    "PDT_values = [1]\n",
    "run_experiments(data_with_precision, models, EB_values, CTX_values, PDT_values, test_len=None, results_file_path=\"all_data_results.csv\",create_new_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6930086-ccf8-4ca2-a8ac-ea15e15bf2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
